---
output: github_document
title: "constRuct"
editor_options: 
  chunk_output_type: console
---

```{bash, include = FALSE, eval = FALSE}
export AZURITE_ACCOUNTS="my-test-account:bXlhd2Vzb21lcGFzc3dvcmQ"
```

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

`constRuct` provides a simple yet powerful object-oriented framework for creating stateful and reusable ETL (Extract, Transform, Load) pipelines in R. It uses R6 classes to encapsulate parameters, data sets, and logs for each pipeline run, promoting code that is clean, modular, and easy to maintain.
It includes a base class for core functionality and an example of an inherited class for interacting with Microsoft Azure Blob Storage.

## Features

-   **Stateful Objects**: Each pipeline run is an object that holds its own parameters, datasets, and logs.
-   **Built-in Logging**: Automatically timestamped logging is available for every step of your process.
-   **Parameter Management**: Easily pass and store parameters for a pipeline run.
-   **Extensible by Design**: Inherit from `PipelineBase` to create specialized pipelines for different data sources or services (e.g., databases, other cloud providers, APIs).
-   **Azure Integration**: The `AzurePipe` class provides ready-to-use methods for connecting to Azure Blob Storage to list containers and upload files.

## Installation

You can install the development version of `constRuct` from GitHub with:

```{r, eval = FALSE}
# install.packages("remotes")
remotes::install_github("your-username/constRuct")
```

## Setup for Azure Integration

To use the `AzurePipe` class, you must provide storage credentials as environment variables. The recommended way to do this is by adding them to an `.Renviron` file in your project's root directory. Use `usethis::edit_r_environ()` to easily open and edit this file.

Your `.Renviron` file should contain:

```{r eval=FALSE, echo=TRUE}
# This example uses the 'Azurite' emaulator
AZURE_BLOB_ENDPOINT="http://127.0.0.1:10000/devstoreaccount1"
AZURE_KEY="Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw=="
```

## Core Concepts: `PipelineBase`

The foundation of the package is `PipelineBase`. It provides the core functionality for logging and managing datasets within a pipeline object.

1.  Initialize a pipeline object

```{r}
library(constRuct)
library(glue)

# After loading the package, the R6 class generator is available directly.
local_pipeline <- constRuct::PipelineBase$new(
  pipeline_name = "Local Data Processing",
  pipeline_id = "data-processing",
  division = "Research",
  program = "Program"
)

```

2.  Add and retrieve datasets from the object's internal state

```{r}

project_sites <- data.frame(
  site_id = c("A-1", "B-2", "C-3"),
  location = c("San Diego", "New York", "Chicago")
)

local_pipeline$add_dataset(name = "sites", data = project_sites)
retrieved_data <- local_pipeline$get_dataset("sites")

print(head(retrieved_data))

```

3.  View the execution log at any time

```{r}
# All steps are automatically logged with timestamps.
print(local_pipeline$get_logs())
```

## Extending the Framework: AzurePipe

`AzurePipe` inherits all the methods of `PipelineBase` and adds new capabilities for interacting with Azure Blob Storage. This example demonstrates how to initialize the pipeline, interact with Azure, and upload the pipeline's own log file.

1.  Initialize the Azure-aware pipeline

```{r}

# The constructor automatically connects to your Azure endpoint.
azure_pipeline <- AzurePipe$new(
  pipeline_name = "Daily Report to Azure",
  pipeline_id = "daily-azure-report",
  division = "Finance",
  program = "Expenses"
)

```

2.  List available containers using the `get_containers` active binding

```{r}
cat("\n--- Available Containers in Azure ---\n")
containers <- azure_pipeline$get_containers
print(sapply(containers, function(c) c$name))
```

3.  Run a process using inherited methods from `PipelineBase`

```{r}
azure_pipeline$add_log("Generating daily summary data.")

daily_summary <- data.frame(
  date = Sys.Date(),
  metric = "user_signups",
  value = round(runif(1, 100, 500))
)
azure_pipeline$add_dataset("summary_data", daily_summary)

```

4.  Use a specialized method to upload the log to Azure

```{r}

azure_pipeline$upload_log_file(
  file_content = azure_pipeline$get_logs()
)
```

5.  Verify the upload by listing blobs

```{r}

cat("\n--- Verifying Upload in 'logs' Container ---\n")
print(azure_pipeline$list_blobs("logs"))

```
